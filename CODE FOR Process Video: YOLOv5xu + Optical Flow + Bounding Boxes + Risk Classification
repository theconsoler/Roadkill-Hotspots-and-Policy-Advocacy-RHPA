import cv2
import numpy as np
from ultralytics import YOLO

# Load YOLOv5 model
model = YOLO("C:/Users/ASUS/Downloads/JUPYTER PROJECTS/RHPA PROJECT/yolov5xu.pt")

# Load input video
video_path = "C:/Users/ASUS/Downloads/dead racoon.mp4"
cap = cv2.VideoCapture(video_path)

# Get video properties
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
fps = int(cap.get(cv2.CAP_PROP_FPS))

# Define codec and create VideoWriter object
output_path = "output_video.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Read the first frame
ret, prev_frame = cap.read()
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

# Process video frame-by-frame
while cap.isOpened():
    ret, next_frame = cap.read()
    if not ret:
        break

    next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)

    # Compute Optical Flow (Farneback method)
    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
    movement = np.mean(np.abs(flow))

    # Determine Alive or Dead
    motion_status = "Alive" if movement > 0.1 else "Potentially Dead"

    # Detect animals with YOLO
    results = model(next_frame)

    for result in results:
        boxes = result.boxes.xyxy.cpu().numpy()  # Get bounding boxes
        confidences = result.boxes.conf.cpu().numpy()  # Get confidence scores
        class_ids = result.boxes.cls.cpu().numpy()  # Get class IDs

        for box, conf, class_id in zip(boxes, confidences, class_ids):
            x_min, y_min, x_max, y_max = map(int, box)

            # Assign risk level
            risk_level = classify_risk(box, frame_width)

            # Draw bounding box
            color = (0, 255, 0) if motion_status == "Alive" else (0, 0, 255)  # Green for alive, Red for dead
            cv2.rectangle(next_frame, (x_min, y_min), (x_max, y_max), color, 2)

            # Display status and risk level
            label = f"{motion_status}, {risk_level} ({conf:.2f})"
            cv2.putText(next_frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # Write processed frame to output video
    out.write(next_frame)

    # Update previous frame for next iteration
    prev_gray = next_gray

cap.release()
out.release()

print(f"Processed video saved at: {output_path}")
